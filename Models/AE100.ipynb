{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from scipy.io import wavfile\n",
    "from torch.utils.data import Dataset\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, Flatten, Dense, Input, Dropout, BatchNormalization, Reshape, Conv1DTranspose, UpSampling1D, UpSampling2D, MaxPooling1D, UpSampling1D, Layer, Embedding, Add, Multiply\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Concatenate\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, LeakyReLU\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import time\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 50\n",
    "TRAINING_DATA_AMOUNT = 100000\n",
    "SAMPLE_RATE = 24000\n",
    "FRAME_LENGTH = int(SAMPLE_RATE * 0.02)\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "class DataSetLoader():\n",
    "    def __init__(self,\n",
    "                dev_data_set_path='dev-clean/',\n",
    "                train_data_set_path='train-clean-360/',\n",
    "                test_data_set_path='test-clean/'):\n",
    "        self.dev_data_set_path = dev_data_set_path\n",
    "        self.train_data_set_path = train_data_set_path\n",
    "        self.test_data_set_path = test_data_set_path\n",
    "        self.file_names = []\n",
    "\n",
    "    def load_data_set(self, data_set_path):\n",
    "        for subdir, dirs, files in os.walk(data_set_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file.split(\".\")[1] == \"wav\":\n",
    "                    self.file_names.append(file_path)\n",
    "\n",
    "        print(\"Loaded in: \\n- wave_files:\", len(self.file_names))\n",
    "\n",
    "        return self.file_names\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_to_frames(sample_data, frame_length=FRAME_LENGTH, frame_step=SAMPLE_RATE):\n",
    "        return tf.signal.frame(sample_data, frame_length=frame_length, frame_step=frame_step, pad_end=True, pad_value=0)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return wavfile.read(self.file_names[index])\n",
    "\n",
    "dataset_loader = DataSetLoader()\n",
    "dataset_files = dataset_loader.load_data_set(dataset_loader.train_data_set_path)\n",
    "dataset_files = dataset_files[:TRAINING_DATA_AMOUNT]\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset_files, [int(TRAINING_DATA_AMOUNT * 0.8), int(TRAINING_DATA_AMOUNT * 0.2)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Initialize WandB and other configurations\n",
    "for PACKET_LOSS_PERCENTAGE in [0.8, 0.6, 0.4, 0.2, 0.0]:\n",
    "    wandb.init(\n",
    "        project=\"FINAL_AE100_SWEEPING_PACKET_LOSS_ZEROS\",\n",
    "        config={\n",
    "            \"architecture\": \"AutoEncoder\",\n",
    "            \"dataset\": \"LibriTTS Corpus\",\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"training_data_amount\": TRAINING_DATA_AMOUNT,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"packet_loss_percentage\": PACKET_LOSS_PERCENTAGE,\n",
    "            \"model_complexity\": \"100percent latent\",\n",
    "            \"frame_length\": FRAME_LENGTH,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Define the autoencoder model\n",
    "    class MyAutoEncoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MyAutoEncoder, self).__init__()\n",
    "            # Define the layers of the autoencoder\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "                nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "                nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose1d(in_channels=8, out_channels=16, kernel_size=2, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=2, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose1d(in_channels=32, out_channels=1, kernel_size=2, stride=2),\n",
    "                nn.Tanh() # Mapping the output to [-1, 1] as the input is also in this range\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Pass input through encoder\n",
    "            x = self.encoder(x)\n",
    "\n",
    "            # Simulate packet loss at the defined packet loss percentage\n",
    "            for batch in x:\n",
    "                for packet in batch:\n",
    "                    if torch.rand(1) < PACKET_LOSS_PERCENTAGE:\n",
    "                        packet = torch.full_like(packet, -1)\n",
    "            \n",
    "            # Pass the output through the decoder\n",
    "            x = self.decoder(x)\n",
    "            return x\n",
    "\n",
    "    # Set device (GPU if available, else CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create an instance of the autoencoder\n",
    "    autoencoder = MyAutoEncoder().to(device)\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    def plot_validation_data():\n",
    "        for i in range(5):\n",
    "            input, sr = torchaudio.load(val_dataset[i])\n",
    "            input = torch.unsqueeze(input, 0).to(device)\n",
    "            output = autoencoder(input)\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(input.squeeze().detach().cpu().numpy(), label=\"Input\")\n",
    "            plt.plot(output.squeeze().detach().cpu().numpy(), label=\"Output\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            ipd.display(ipd.Audio(input.squeeze().detach().cpu().numpy(), rate=sr))\n",
    "            ipd.display(ipd.Audio(output.squeeze().detach().cpu().numpy(), rate=sr))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for data in train_dataset:\n",
    "            inputs, sr = torchaudio.load(data)\n",
    "            inputs = torch.split(inputs, FRAME_LENGTH, dim=1)\n",
    "\n",
    "            # If the last input is not the correct size, remove it\n",
    "            if inputs[-1].size(1) != FRAME_LENGTH:\n",
    "                inputs = inputs[:-1]\n",
    "\n",
    "            for i in range(0, len(inputs), BATCH_SIZE):\n",
    "                # Gather the inputs for the batch\n",
    "                inputs_batch = torch.stack(inputs[i:i+BATCH_SIZE]).to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = autoencoder(inputs_batch)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, inputs_batch[:, :, :outputs.size(2)])\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Log the loss\n",
    "                running_loss += loss.item() * inputs_batch.size(0)\n",
    "        \n",
    "        training_loss = running_loss / len(train_dataset)\n",
    "        validation_loss = 0.0\n",
    "        for data in val_dataset:\n",
    "            inputs, sr = torchaudio.load(data)\n",
    "            inputs = torch.unsqueeze(inputs, 0).to(device)\n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs[:, :, :outputs.size(2)])\n",
    "            validation_loss += loss.item() * inputs.size(0)\n",
    "        wandb.log({\"train_loss\": training_loss, \"val_loss\": validation_loss, \"epoch\": epoch})\n",
    "\n",
    "    # Save 5 samples of the validation data\n",
    "    for i in range(5):\n",
    "        input, sr = torchaudio.load(val_dataset[i])\n",
    "        output = autoencoder(input.unsqueeze(0))\n",
    "\n",
    "        print(f\"Packet Loss Percentage: {PACKET_LOSS_PERCENTAGE}\")\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(input.squeeze().detach().cpu().numpy())\n",
    "        plt.plot(output.squeeze().detach().cpu().numpy())\n",
    "        plt.show()\n",
    "\n",
    "        ipd.display(ipd.Audio(input.squeeze().detach().cpu().numpy(), rate=sr))\n",
    "        ipd.display(ipd.Audio(output.squeeze().detach().cpu().numpy(), rate=sr))\n",
    "        print(\"Validation Loss: \", loss)\n",
    "        print(\"--------------------\")\n",
    "    \n",
    "    # Calculating the inference time for a frame\n",
    "    input, sr = torchaudio.load(val_dataset[0])\n",
    "    input = input[:, :FRAME_LENGTH]\n",
    "    start_time = time.time()\n",
    "    output = autoencoder(input.unsqueeze(0))\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    print(\"Inference Time: \", inference_time)\n",
    "\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"Finished Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
