{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from scipy.io import wavfile\n",
    "from torch.utils.data import Dataset\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, Flatten, Dense, Input, Dropout, BatchNormalization, Reshape, Conv1DTranspose, UpSampling1D, UpSampling2D, MaxPooling1D, UpSampling1D, Layer, Embedding, Add, Multiply\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Concatenate\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, LeakyReLU\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import time\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 50\n",
    "TRAINING_DATA_AMOUNT = 100000\n",
    "SAMPLE_RATE = 24000\n",
    "FRAME_LENGTH = int(SAMPLE_RATE * 0.02)\n",
    "LEARNING_RATE = 0.001\n",
    "WANDB_LOG = True\n",
    "\n",
    "NUM_LATENTS = 16\n",
    "LATENT_DIM = 16\n",
    "COMMITMENT_COST = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from vector_quantize_pytorch import VectorQuantize\n",
    "\n",
    "\n",
    "def plot_validation_data():\n",
    "    for i in range(5):\n",
    "        input, sr = torchaudio.load(val_dataset[i])\n",
    "        input = torch.unsqueeze(input, 0)\n",
    "        output = autoencoder(input)\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(input.squeeze().detach().cpu().numpy())\n",
    "        plt.plot(output.squeeze().detach().cpu().numpy())\n",
    "        plt.show()\n",
    "\n",
    "        ipd.display(ipd.Audio(input.squeeze().detach().cpu().numpy(), rate=sr))\n",
    "        ipd.display(ipd.Audio(output.squeeze().detach().cpu().numpy(), rate=sr))\n",
    "\n",
    "\n",
    "class DataSetLoader():\n",
    "    def __init__(self,\n",
    "                dev_data_set_path='dev-clean/',\n",
    "                train_data_set_path='train-clean-360/',\n",
    "                test_data_set_path='test-clean/'):\n",
    "        self.dev_data_set_path = dev_data_set_path\n",
    "        self.train_data_set_path = train_data_set_path\n",
    "        self.test_data_set_path = test_data_set_path\n",
    "        self.file_names = []\n",
    "\n",
    "    def load_data_set(self, data_set_path):\n",
    "        for subdir, dirs, files in os.walk(data_set_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file.split(\".\")[1] == \"wav\":\n",
    "                    self.file_names.append(file_path)\n",
    "\n",
    "        print(\"Loaded in: \\n- wave_files:\", len(self.file_names))\n",
    "\n",
    "        return self.file_names\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return wavfile.read(self.file_names[index])\n",
    "\n",
    "\n",
    "dataset_loader = DataSetLoader()    \n",
    "dataset_files = dataset_loader.load_data_set(dataset_loader.train_data_set_path)\n",
    "dataset_files = dataset_files[:TRAINING_DATA_AMOUNT]\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset_files, [int(TRAINING_DATA_AMOUNT * 0.8), int(TRAINING_DATA_AMOUNT * 0.2)])\n",
    "for PACKET_LOSS_PERCENTAGE in [0.0, 0.2, 0.4, 0.6, 0.8]:\n",
    "    LATENT_DIM = NUM_LATENTS\n",
    "    if WANDB_LOG:\n",
    "        wandb.init(\n",
    "            project=\"FINAL_VQ_SWEEPING_PACKET_LOSS_TEST\",\n",
    "            config={\n",
    "            \"architecture\": \"AutoEncoderVQ_PACKET_LOSS\",\n",
    "            \"dataset\": \"LibriTTS Corpus\",\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"training_data_amount\": TRAINING_DATA_AMOUNT,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"model_complexity\": \"25percent latent VQ\",\n",
    "            \"packet_loss_percentage\": PACKET_LOSS_PERCENTAGE,\n",
    "            \"frame_length\": FRAME_LENGTH,\n",
    "            \"num_latents\": NUM_LATENTS,\n",
    "            \"latent_dim\": LATENT_DIM,\n",
    "            \"commitment_cost\": COMMITMENT_COST,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    # Define the autoencoder model with VQ\n",
    "    class MyAutoEncoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MyAutoEncoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "                nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "                nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "                nn.Conv1d(in_channels=8, out_channels=LATENT_DIM, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            )\n",
    "            # Vector quantization layer defined with specified number of latents\n",
    "            self.quantize = VectorQuantize(\n",
    "                dim=LATENT_DIM, \n",
    "                codebook_size=NUM_LATENTS, \n",
    "                decay=0.8, \n",
    "                kmeans_init=True,\n",
    "                kmeans_iters=10,\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose1d(in_channels=NUM_LATENTS, out_channels=8, kernel_size=2, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose1d(in_channels=8, out_channels=16, kernel_size=2, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=2, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose1d(in_channels=32, out_channels=1, kernel_size=2, stride=2),\n",
    "                nn.Tanh() # Mapping the output to [-1, 1] as the input is also in this range\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Pass input through encoder\n",
    "            x = self.encoder(x)\n",
    "            # Save the shape for reshaping after vector quantization\n",
    "            original_shape = x.shape\n",
    "            # Reshape for vector quantization\n",
    "            x = x.view(-1, original_shape[1])\n",
    "            # Apply vector quantization\n",
    "            quantized, indices, commit_loss = self.quantize(x)\n",
    "\n",
    "            # Introduce packet loss to each row independently based on PACKET_LOSS_PERCENTAGE\n",
    "            for i in range(quantized.shape[0]):\n",
    "                if torch.rand(1) < PACKET_LOSS_PERCENTAGE:\n",
    "                    quantized[i] = torch.zeros(quantized[i].shape)\n",
    "\n",
    "            # Reshape back to the original shape\n",
    "            quantized = quantized.view(*original_shape)\n",
    "\n",
    "            # Pass encoded representation through decoder\n",
    "            x = self.decoder(quantized)\n",
    "            return x, commit_loss\n",
    "\n",
    "    # Set device (GPU if available, else CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create an instance of the autoencoder with VQ\n",
    "    autoencoder = MyAutoEncoder().to(device)\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for data in train_dataset:\n",
    "            inputs, sr = torchaudio.load(data)\n",
    "\n",
    "            # Split the input into frames\n",
    "            inputs = torch.split(inputs, FRAME_LENGTH, dim=1)\n",
    "\n",
    "            # Remove the last frame if it is not the correct size\n",
    "            if inputs[-1].size(1) != FRAME_LENGTH:\n",
    "                inputs = inputs[:-1]\n",
    "\n",
    "            num_batches = len(inputs) // BATCH_SIZE\n",
    "\n",
    "            for i in range(0, len(inputs), BATCH_SIZE):\n",
    "                # Create a batch of inputs\n",
    "                inputs_batch = torch.stack(inputs[i:i+BATCH_SIZE])\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs, commit_loss = autoencoder(inputs_batch)\n",
    "\n",
    "\n",
    "                # Calculate the loss\n",
    "                reconstruction_loss = criterion(outputs, inputs_batch)\n",
    "                loss = reconstruction_loss  + COMMITMENT_COST * commit_loss\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.step()\n",
    "\n",
    "                # Log the loss\n",
    "                running_loss += loss.item() * inputs_batch.size(0)\n",
    "        scheduler.step()\n",
    "\n",
    "        validation_loss = 0.0\n",
    "        for data in val_dataset:\n",
    "            inputs, sr = torchaudio.load(data)\n",
    "            inputs = torch.unsqueeze(inputs, 0).to(device)\n",
    "            outputs, commit_loss = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs[:, :, :outputs.size(2)])\n",
    "            validation_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        \n",
    "        running_loss /= len(train_dataset)\n",
    "        print(f\"Epoch {epoch}, Loss: {running_loss:.6f}\", f\"Validation Loss: {validation_loss:.6f}\")\n",
    "        print(\"Average Validation Loss: \", validation_loss / len(val_dataset))\n",
    "\n",
    "        if WANDB_LOG:\n",
    "            wandb.log({\"epoch\": epoch, \"train_loss\": running_loss, \"val_loss\": validation_loss})\n",
    "\n",
    "    for i in range(5):\n",
    "        # Log the inference time\n",
    "        input, sr = torchaudio.load(val_dataset[i])\n",
    "        start_time = time.time()\n",
    "        output, vq_loss = autoencoder(input.unsqueeze(0))\n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Packet Loss Percentage: {PACKET_LOSS_PERCENTAGE}\")\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(input.squeeze().detach().cpu().numpy())\n",
    "        plt.plot(output.squeeze().detach().cpu().numpy())\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        ipd.display(ipd.Audio(input.squeeze().detach().cpu().numpy(), rate=sr))\n",
    "        ipd.display(ipd.Audio(output.squeeze().detach().cpu().numpy(), rate=sr))\n",
    "        print(\"--------------------\")\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"Finished Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
